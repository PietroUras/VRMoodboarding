## ğŸ¨ Project Overview

This project presents a **Virtual Reality (VR) moodboarding tool** with a multi-layered software architecture designed for **intuitive multimodal interaction**.  
It leverages a combination of modern development tools, open standards, and cloud services to deliver a flexible and high-performance immersive experience.

---

## âš™ï¸ Core Development Stack

The application is built with:

- **Unity** â€“ lightweight and optimized for VR hardware performance.
- **OpenXR** â€“ ensures cross-device compatibility and avoids vendor lock-in.
- **Mixed Reality Toolkit 3 (MRTK3)** â€“ provides advanced hand tracking, a library of pre-built components, and an XR Simulator for in-editor testing.

---

## ğŸ§© Multimodal Interaction & UX Design

The system employs a **hybrid interaction model**:  
ğŸ’¡ *Virtual buttons + Gesture-driven interface*, guided by a **"Hand Tracking First"** philosophy.

Key UX principles:

- ğŸ§± **Skeuomorphism**: Real-world metaphors (e.g., corkboard).
- ğŸ§  **Gestalt Principles**: Organized layout to reduce cognitive load.
- âœ¨ **Progressive Disclosure**: On-demand UI elements to avoid clutter.

---

## âœ‹ Gesture-Driven Interface & Recognition

Custom gestures are central to the user experience. Designed through **research, prototyping, and user testing**, gestures are:

- **Learnable**, **memorable**, and **ergonomically sound**.
- Recognized using **XR Hands**, Unityâ€™s official hand tracking system.

### ğŸ¯ Gesture Categories

| ğŸ”¹ **Trigger Gestures**                | ğŸ”¹ **UI Navigation Gestures**         |
|--------------------------------------|--------------------------------------|
| **Start Mic** _(fingers to mouth)_<br>Begins vocal input <br><img src="./gifs/mic_gesture.gif" alt="Mic Gesture" width="400"/> | **Thumbs Up** <br>Quick confirmation <br><img src="./gifs/yes_gesture.gif" alt="Yes Gesture" width="400"/> |
| **Frame** _(hands form rectangle)_<br>Creates new moodboard/image <br><img src="./gifs/frame_gesture.gif" alt="Frame Gesture" width="400"/> | **Swipe** <br>Directional navigation <br><img src="./gifs/create_swipe.gif" alt="Swipe Gesture" width="400"/> |


## ğŸ§  AI-Based Image Generation

The system integrates **Diffusion models** via APIs like:

- ğŸ§ª **Hugging Face**
- âš¡ **NScale**

### ğŸ”€ Prompt Composition

Image prompts are built from:

1. ğŸ™ï¸ **Vocal Input** â€“ userâ€™s creative idea
2. ğŸ—’ï¸ **Project Brief** â€“ contextual info (tone, style)
3. ğŸ›ï¸ **GUI Parameters** â€“ lighting, color, aspect ratio

Multiple image candidates are generated per prompt to **reduce ambiguity** and increase creative control.

---

## ğŸ§± Software Architecture Patterns

Designed for **maintainability and scalability** using:

- ğŸ”„ **Observer Pattern** â€“ decouples event handling
- ğŸ”€ **MVVM** â€“ separates UI, logic, and data
- ğŸ”’ **Singletons** â€“ for core services (e.g., `SpeechToTextManager`)
- ğŸ§© **Image Generation Interface** â€“ abstracts API backends for testing/production
- ğŸ‘ï¸ **Gaze-Based Positioning** â€“ recenters UI panels to user's FOV during scene changes

---

## ğŸ§ª UX Prototyping & Workflow

The design process started with **Figma-based prototyping**, using:

- ğŸ¨ **MRTK3 Figma Toolkit** â€“ prebuilt components optimized for VR ergonomics
- ğŸ“ Constraints like **field of view** and **reach zones** respected from early stages

### ğŸ“ Comfort Zones

Although Figma is a 2D tool, the interface layout was designed to respect VR-specific constraints such as **field of view (FOV)**, **depth**, and **ergonomic reach**.  
The working canvas was set to **3664 Ã— 1920 pixels**, simulating the combined resolution of the Meta Quest 2 headset (*1832 Ã— 1920 pixels per eye*).

**Visibility and comfort zones** were calculated based on pixel-per-degree metrics from VR usability studies, and further informed by the Figma community resource *â€œGuide for Spatial Design of VRâ€*, adapted to match the optical properties of the Meta Quest 2.

#### Zones definition:

- ğŸŸ¢ **Green Zone** (central clarity, ~50Â° FOV): **1000 Ã— 1000 px**
- ğŸŸ¡ **Yellow Zone** (peripheral area, ~50Â°â€“90Â°): **1600 Ã— 1600 px**
- âšª **Gray Zone** (outside ~90Â°): areas discouraged for UI placement

#### Head movement comfort boundaries:

- ğŸ”„ Horizontal neck rotation (Â±30Â°): **Â±1000 px**
- ğŸ”¼ Vertical head tilt (Â±20Â°): **Â±700 px**

<img src="./Images/figma_prototype/Figma_comfortZones.jpg" alt="Comfort Zones in VR UI" width="600"/>

### â¡ï¸ Workflow:  
**Figma** â†’ rapid iteration â†’ **Unity** implementation â†’ in-headset testing  
This ensured a final interface that is both **aesthetically polished** and **spatially ergonomic**.

### ğŸ“ Project Selection Mockup with comfort boundaries

<img src="./Images/figma_prototype/Figma_projectsSelectionMockup.png" alt="Figma mockup of the project selection screen" width="600"/>

### ğŸ§© Moodboard Interface Mockup

<img src="./Images/figma_prototype/Figma_moodboardoMockup.png" alt="Figma mockup of the moodboard interface" width="600"/>

---

## ğŸ§ª Final Implementation in Unity

The following screenshots show the actual implementation of the system in Unity, translating the Figma-designed concepts into a working VR environment:

<div align="center">

<table>
  <tr>
    <th>âœ‹ Grabbing Board</th>
    <th>ğŸ–ï¸ Hand Menu</th>
  </tr>
  <tr>
    <td><img src="./Images/app_screenshots/Unity_grabbingBoard.png" alt="Unity Grabbing Board" width="400"/></td>
    <td><img src="./Images/app_screenshots/Unity_HandMenu.png" alt="Unity Hand Menu" width="400"/></td>
  </tr>
  <tr>
    <th>ğŸ–¼ï¸ Image Creation</th>
    <th>ğŸ” Image Selection and Details</th>
  </tr>
  <tr>
    <td><img src="./Images/app_screenshots/Unity_imageCreation.png" alt="Unity Image Creation" width="400"/></td>
    <td><img src="./Images/app_screenshots/Unity_imageDetails.png" alt="Unity Image Details" width="400"/></td>
  </tr>
</table>

</div>

These views highlight:

* ğŸ§© **Consistency with Figma layouts**
* ğŸ‘ **Fully functional gesture and button interactions**
* ğŸ¯ **Refined user interface aligned with ergonomic constraints**



